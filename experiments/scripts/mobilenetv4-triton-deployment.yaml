apiVersion: v1
kind: Namespace
metadata:
  name: workloads
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mobilenetv4-model-pvc
  namespace: workloads
spec:
  accessModes:
    - ReadWriteOnce # Or ReadOnlyMany if multiple Triton instances might read from it, though usually 1 PVC per instance or use ReadWriteOnce.
  resources:
    requests:
      storage: 5Gi # Adjust based on your model(s) size
  # storageClassName: microk8s-hostpath # If you need a specific storage class in MicroK8s and it's not the default
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: mobilenetv4-config-pbtxt-cm
  namespace: workloads
data:
  config.pbtxt: |
    name: "mobilenetv4"
    platform: "onnxruntime_onnx" # Or "tensorflow_savedmodel" or "tensorrt_plan"
    max_batch_size: 0 # 0 means dynamic batching is supported by the model and Triton's scheduler
    input [
      {
        name: "pixel_values" # Corrected input tensor name
        data_type: TYPE_FP32 # Replace with actual data type
        dims: [ -1, 3, 224, 224 ] # Assuming standard ImageNet input [batch, channels, height, width]
      }
    ]
    output [
      {
        name: "output" # Default/common output name, verify with Netron if issues persist
        data_type: TYPE_FP32 # Replace with actual data type
        dims: [ -1, 1000 ] # Assuming 1000 classes for ImageNet, verify with Netron
      }
    ]
    instance_group [ { kind: KIND_GPU, count: 1 } ]
    # dynamic_batching { preferred_batch_size: [4, 8], max_queue_delay_microseconds: 100 } # Optional: configure dynamic batching
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mobilenetv4-triton-deployment
  namespace: workloads
  labels:
    app: mobilenetv4-triton
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mobilenetv4-triton
  template:
    metadata:
      labels:
        app: mobilenetv4-triton
    spec:
      # Optional: If your MicroK8s node has taints that prevent scheduling, add tolerations
      # tolerations:
      # - key: "key"
      #   operator: "Exists"
      #   effect: "NoSchedule"
      volumes:
        - name: model-storage-pvc
          persistentVolumeClaim:
            claimName: mobilenetv4-model-pvc
        - name: model-config-cm
          configMap:
            name: mobilenetv4-config-pbtxt-cm
        - name: model-repository # emptyDir volume for the combined model repository
          emptyDir: {}
      initContainers:
      - name: populate-model-repository
        image: busybox:latest # Or any image with cp/mkdir
        command: ['/bin/sh', '-c']
        args:
          - |
            echo "Copying model config (config.pbtxt)..."
            mkdir -p /mnt/repository/mobilenetv4
            cp /mnt/config/config.pbtxt /mnt/repository/mobilenetv4/config.pbtxt
            echo "Model config copied."
            echo "Init container expects model.onnx to be copied by prepare_model_pvc.sh into the PVC."
            echo "Then, the main triton container will mount the PVC at /models."
            echo "The Triton deployment itself points its --model-repository to the PVC mount."
            echo "This init container (populate-model-repository) primarily handles the config.pbtxt."
            echo "If model was also to be copied by this init container from a PVC to an emptyDir, that logic would be here."
            echo "Current design: PVC is directly mounted by Triton main container."
            ls -R /mnt/repository
            # The model itself (model.onnx) is expected to be in the PVC already (copied by model-copy-pod)
            # and Triton directly uses the PVC mount path /models.
            # This init container is simplified to only place config.pbtxt if needed in an emptyDir structure.
            # However, the current Triton deployment mounts the PVC at /models directly, 
            # and the config.pbtxt from configmap is used to construct the model repo by an earlier init container (if used that way)
            # OR triton loads it if config.pbtxt is alongside model.onnx in the PVC.
            # For clarity, let's ensure this init container copies config.pbtxt to the emptyDir, 
            # and the main container mounts BOTH the PVC (for model.onnx) and this emptyDir (for config.pbtxt).
            # Let's adjust: model repository for triton will be an emptyDir populated by initContainers.
            # initContainer1 (this one) copies config.pbtxt to emptyDir/mobilenetv4/config.pbtxt
            # initContainer2 (new or modified) copies model.onnx from PVC to emptyDir/mobilenetv4/1/model.onnx
            # OR, simplify: Main Triton container mounts PVC at /repository_base and its model dir is /repository_base/mobilenetv4.
            # config.pbtxt will be in /repository_base/mobilenetv4/config.pbtxt (copied by this init from CM)
            # model.onnx will be in /repository_base/mobilenetv4/1/model.onnx (copied by an earlier pod into PVC)

            # Simplest approach with current setup: 
            # 1. model-copy-pod (from prepare_model_pvc.sh) copies model.onnx into PVC -> /mobilenetv4/1/model.onnx
            # 2. This init container copies config.pbtxt from ConfigMap into the same PVC structure -> /mobilenetv4/config.pbtxt
            # 3. Triton main container mounts the PVC at /models and uses --model-repository=/models

            echo "Copying config.pbtxt to PVC via an intermediate mount..."
            # This init container needs access to the PVC to place the config.pbtxt.
            # The command in model-copy-pod already created /pvc_mount/mobilenetv4/1. We need /pvc_mount/mobilenetv4.
            mkdir -p /pvc_model_mount/mobilenetv4 
            cp /cm_config/config.pbtxt /pvc_model_mount/mobilenetv4/config.pbtxt
            echo "Copied config.pbtxt to relative PVC path mobilenetv4/config.pbtxt"
            echo "Listing content of where config.pbtxt should be on PVC (via this init container's perspective):"
            ls -lR /pvc_model_mount/mobilenetv4

        volumeMounts:
          - name: model-storage-pvc # Mount the PVC here to write config.pbtxt
            mountPath: /pvc_model_mount 
          - name: model-config-cm # Mount the configMap for config.pbtxt source
            mountPath: /cm_config 
      containers:
      - name: triton-inference-server
        # Check NVIDIA NGC for the latest recommended Triton image tag for general use (e.g., includes ONNX, TF, TensorRT backends)
        # Example tag: nvcr.io/nvidia/tritonserver:24.04-py3 (verify this)
        image: nvcr.io/nvidia/tritonserver:24.04-py3
        command: ["tritonserver"]
        args:
          - --model-repository=/models # This is the emptyDir populated by the init container
          - --strict-model-config=false # Allows Triton to start even if a model initially fails to load
          # - --log-verbose=1 # Uncomment for more detailed logs
        resources:
          limits:
            nvidia.com/gpu: 1
          requests:
            nvidia.com/gpu: 1
        ports:
        - containerPort: 8000
          name: http
        - containerPort: 8001
          name: grpc
        - containerPort: 8002
          name: metrics
        readinessProbe:
          httpGet:
            path: /v2/health/ready
            port: http
          initialDelaySeconds: 20
          periodSeconds: 5
          timeoutSeconds: 2
          failureThreshold: 5
        livenessProbe:
          httpGet:
            path: /v2/health/live
            port: http
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 2
          failureThreshold: 3
        volumeMounts:
          - name: model-storage-pvc # Mount the PVC directly, expects models/mobilenetv4/1/model.onnx & models/mobilenetv4/config.pbtxt
            mountPath: /models 
---
apiVersion: v1
kind: Service
metadata:
  name: mobilenetv4-triton-svc
  namespace: workloads
  labels:
    app: mobilenetv4-triton
spec:
  selector:
    app: mobilenetv4-triton
  ports:
  - name: http
    port: 8000
    targetPort: http
  - name: grpc
    port: 8001
    targetPort: grpc
  - name: metrics
    port: 8002
    targetPort: metrics
  type: ClusterIP # Start with ClusterIP, change to LoadBalancer if external access needed via MetalLB 